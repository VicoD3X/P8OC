{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f191494",
   "metadata": {},
   "source": [
    "# Initialisation du notebook 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f70af054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow : 2.16.1\n",
      "GPU : No\n",
      "Project root : c:\\Users\\vicau\\P8OC\n",
      "Train images : 586\n",
      "Train masks  : 586\n",
      "Initialisation Notebook 02 : OK\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------\n",
    "# Informations système\n",
    "# ---------------------------\n",
    "print(\"TensorFlow :\", tf.__version__)\n",
    "print(\"GPU :\", \"Yes\" if tf.config.list_physical_devices('GPU') else \"No\")\n",
    "\n",
    "# Fixer une seed (optionnel)\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# ---------------------------\n",
    "# Chemins principaux du projet\n",
    "# ---------------------------\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "DATA_PROCESSED = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "\n",
    "print(\"Project root :\", PROJECT_ROOT)\n",
    "\n",
    "# Dossiers train/val/test\n",
    "TRAIN_IMAGES = DATA_PROCESSED / \"images\" / \"train\"\n",
    "VAL_IMAGES   = DATA_PROCESSED / \"images\" / \"val\"\n",
    "TEST_IMAGES  = DATA_PROCESSED / \"images\" / \"test\"\n",
    "\n",
    "TRAIN_MASKS = DATA_PROCESSED / \"masks\" / \"train\"\n",
    "VAL_MASKS   = DATA_PROCESSED / \"masks\" / \"val\"\n",
    "TEST_MASKS  = DATA_PROCESSED / \"masks\" / \"test\"\n",
    "\n",
    "# Vérification minimale\n",
    "assert TRAIN_IMAGES.exists(), \"Images train manquantes\"\n",
    "assert TRAIN_MASKS.exists(),  \"Masks train manquants\"\n",
    "\n",
    "print(\"Train images :\", len(list(TRAIN_IMAGES.glob('*.png'))))\n",
    "print(\"Train masks  :\", len(list(TRAIN_MASKS.glob('*.png'))))\n",
    "\n",
    "# ---------------------------\n",
    "# Import du générateur\n",
    "# ---------------------------\n",
    "sys.path.append(str(PROJECT_ROOT / \"src\"))\n",
    "from dataloader import CityscapesSequence\n",
    "\n",
    "# ---------------------------\n",
    "# Configuration affichage\n",
    "# ---------------------------\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 4)\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "\n",
    "print(\"Initialisation Notebook 02 : OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4e40ee",
   "metadata": {},
   "source": [
    "## Chargement des DataGenerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "241d8b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples : 586\n",
      "Val samples   : 267\n",
      "Test samples  : 544\n",
      "Train batches : 146\n",
      "Val batches   : 66\n",
      "Bloc DataGenerators : OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vicau\\P8OC\\.venv\\Lib\\site-packages\\albumentations\\core\\validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Récupération des chemins des images et masks\n",
    "train_image_paths = sorted(list(TRAIN_IMAGES.glob(\"*.png\")))\n",
    "val_image_paths   = sorted(list(VAL_IMAGES.glob(\"*.png\")))\n",
    "test_image_paths  = sorted(list(TEST_IMAGES.glob(\"*.png\")))\n",
    "\n",
    "train_mask_paths = sorted(list(TRAIN_MASKS.glob(\"*.png\")))\n",
    "val_mask_paths   = sorted(list(VAL_MASKS.glob(\"*.png\")))\n",
    "test_mask_paths  = sorted(list(TEST_MASKS.glob(\"*.png\")))\n",
    "\n",
    "print(\"Train samples :\", len(train_image_paths))\n",
    "print(\"Val samples   :\", len(val_image_paths))\n",
    "print(\"Test samples  :\", len(test_image_paths))\n",
    "\n",
    "# Paramètres généraux\n",
    "BATCH_SIZE = 4\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 512\n",
    "N_CLASSES = 8\n",
    "\n",
    "# Générateurs\n",
    "train_gen = CityscapesSequence(\n",
    "    image_paths=train_image_paths,\n",
    "    mask_paths=train_mask_paths,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    augment=True,\n",
    "    n_classes=N_CLASSES,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_gen = CityscapesSequence(\n",
    "    image_paths=val_image_paths,\n",
    "    mask_paths=val_mask_paths,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    augment=False,\n",
    "    n_classes=N_CLASSES,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(\"Train batches :\", len(train_gen))\n",
    "print(\"Val batches   :\", len(val_gen))\n",
    "print(\"Bloc DataGenerators : OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed349e2",
   "metadata": {},
   "source": [
    "## Import et instanciation des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d21f021",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vicau\\P8OC\\src\\models\\unet_mobilenetv2.py:27: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base = MobileNetV2(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèles instanciés : ['unet_mini', 'unet_vgg16', 'unet_resnet50', 'unet_mobilenetv2']\n",
      "- unet_mini | input: (None, 256, 512, 3) -> output: (None, 256, 512, 8)\n",
      "- unet_vgg16 | input: (None, 256, 512, 3) -> output: (None, 256, 512, 8)\n",
      "- unet_resnet50 | input: (None, 256, 512, 3) -> output: (None, 256, 512, 8)\n",
      "- unet_mobilenetv2 | input: (None, 256, 512, 3) -> output: (None, 256, 512, 8)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# S'assurer que la racine du projet est dans le PYTHONPATH\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# Import des modèles depuis src/models\n",
    "from src.models.unet_mini import unet_mini\n",
    "from src.models.unet_vgg16 import unet_vgg16\n",
    "from src.models.unet_resnet50 import unet_resnet50\n",
    "from src.models.unet_mobilenetv2 import unet_mobilenetv2\n",
    "\n",
    "# Instanciation des 4 modèles\n",
    "# (les paramètres par défaut sont cohérents avec IMG_HEIGHT / IMG_WIDTH / N_CLASSES)\n",
    "model_unet_mini        = unet_mini()\n",
    "model_unet_vgg16       = unet_vgg16()\n",
    "model_unet_resnet50    = unet_resnet50()\n",
    "model_unet_mobilenetv2 = unet_mobilenetv2()\n",
    "\n",
    "# Dictionnaire pratique pour la suite (training / évaluation en boucle)\n",
    "models_dict = {\n",
    "    \"unet_mini\":        model_unet_mini,\n",
    "    \"unet_vgg16\":       model_unet_vgg16,\n",
    "    \"unet_resnet50\":    model_unet_resnet50,\n",
    "    \"unet_mobilenetv2\": model_unet_mobilenetv2,\n",
    "}\n",
    "\n",
    "print(\"Modèles instanciés :\", list(models_dict.keys()))\n",
    "for name, model in models_dict.items():\n",
    "    print(f\"- {name} | input: {model.input_shape} -> output: {model.output_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc198d81",
   "metadata": {},
   "source": [
    "# Entraînement des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20892b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Début de l'entraînement des modèles (hors VGG16) ===\n",
      "\n",
      "----- Entraînement du modèle : unet_mini (epochs = 3) -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vicau\\P8OC\\.venv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 579ms/step - dice_coef: 0.1512 - iou_metric: 0.0262 - loss: 0.8488 - pixel_accuracy: 0.1504 - val_dice_coef: 0.1426 - val_iou_metric: 0.0273 - val_loss: 0.8574 - val_pixel_accuracy: 0.1426\n",
      "Epoch 2/3\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 604ms/step - dice_coef: 0.1202 - iou_metric: 0.0193 - loss: 0.8798 - pixel_accuracy: 0.1202 - val_dice_coef: 0.1426 - val_iou_metric: 0.0273 - val_loss: 0.8574 - val_pixel_accuracy: 0.1426\n",
      "Epoch 3/3\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 602ms/step - dice_coef: 0.1224 - iou_metric: 0.0230 - loss: 0.8776 - pixel_accuracy: 0.1224 - val_dice_coef: 0.1426 - val_iou_metric: 0.0273 - val_loss: 0.8574 - val_pixel_accuracy: 0.1426\n",
      "Modèle sauvegardé : c:\\Users\\vicau\\P8OC\\models\\unet_mini.keras\n",
      "\n",
      "----- Entraînement du modèle : unet_mobilenetv2 (epochs = 3) -----\n",
      "Epoch 1/3\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 639ms/step - dice_coef: 0.4669 - iou_metric: 0.1416 - loss: 0.5331 - pixel_accuracy: 0.4765 - val_dice_coef: 0.5519 - val_iou_metric: 0.2062 - val_loss: 0.4481 - val_pixel_accuracy: 0.5546\n",
      "Epoch 2/3\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 627ms/step - dice_coef: 0.5433 - iou_metric: 0.1884 - loss: 0.4567 - pixel_accuracy: 0.5440 - val_dice_coef: 0.5615 - val_iou_metric: 0.2001 - val_loss: 0.4385 - val_pixel_accuracy: 0.5619\n",
      "Epoch 3/3\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 636ms/step - dice_coef: 0.5888 - iou_metric: 0.2287 - loss: 0.4112 - pixel_accuracy: 0.5899 - val_dice_coef: 0.7421 - val_iou_metric: 0.3733 - val_loss: 0.2579 - val_pixel_accuracy: 0.7435\n",
      "Modèle sauvegardé : c:\\Users\\vicau\\P8OC\\models\\unet_mobilenetv2.keras\n",
      "\n",
      "----- Entraînement du modèle : unet_resnet50 (epochs = 1) -----\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m273s\u001b[0m 2s/step - dice_coef: 0.1632 - iou_metric: 0.0275 - loss: 0.8368 - pixel_accuracy: 0.1642 - val_dice_coef: 0.1426 - val_iou_metric: 0.0273 - val_loss: 0.8574 - val_pixel_accuracy: 0.1426\n",
      "Modèle sauvegardé : c:\\Users\\vicau\\P8OC\\models\\unet_resnet50.keras\n",
      "\n",
      "=== Entraînement terminé pour unet_mini / unet_mobilenetv2 / unet_resnet50 ===\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping   # type: ignore\n",
    "\n",
    "# Nombre d'epochs par modèle (hors VGG16)\n",
    "epochs_plan = {\n",
    "    \"unet_mini\":        3,\n",
    "    \"unet_mobilenetv2\": 3,\n",
    "    \"unet_resnet50\":    1,\n",
    "}\n",
    "\n",
    "history_dict = {}\n",
    "\n",
    "print(\"=== Début de l'entraînement des modèles (hors VGG16) ===\")\n",
    "\n",
    "for name, epochs in epochs_plan.items():\n",
    "    model = models_dict[name]\n",
    "    print(f\"\\n----- Entraînement du modèle : {name} (epochs = {epochs}) -----\")\n",
    "\n",
    "    h = model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    history_dict[name] = h.history\n",
    "\n",
    "    # Sauvegarde du modèle entraîné\n",
    "    save_path = MODELS_DIR / f\"{name}.keras\"\n",
    "    model.save(save_path)\n",
    "\n",
    "    print(f\"Modèle sauvegardé : {save_path}\")\n",
    "\n",
    "print(\"\\n=== Entraînement terminé pour unet_mini / unet_mobilenetv2 / unet_resnet50 ===\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6d8992",
   "metadata": {},
   "source": [
    "## Entrainement du VGG16 (model final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0f971e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Entraînement dédié du modèle unet_vgg16 (epochs = 7) =====\n",
      "Epoch 1/7\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 2s/step - dice_coef: 0.5058 - iou_metric: 0.1633 - loss: 0.4942 - pixel_accuracy: 0.5086 - val_dice_coef: 0.5514 - val_iou_metric: 0.1842 - val_loss: 0.4486 - val_pixel_accuracy: 0.5516 - learning_rate: 1.0000e-04\n",
      "Epoch 2/7\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 2s/step - dice_coef: 0.5511 - iou_metric: 0.1912 - loss: 0.4489 - pixel_accuracy: 0.5516 - val_dice_coef: 0.5617 - val_iou_metric: 0.2021 - val_loss: 0.4383 - val_pixel_accuracy: 0.5625 - learning_rate: 1.0000e-04\n",
      "Epoch 3/7\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 2s/step - dice_coef: 0.5598 - iou_metric: 0.1973 - loss: 0.4402 - pixel_accuracy: 0.5602 - val_dice_coef: 0.5629 - val_iou_metric: 0.2024 - val_loss: 0.4371 - val_pixel_accuracy: 0.5634 - learning_rate: 1.0000e-04\n",
      "Epoch 4/7\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - dice_coef: 0.5660 - iou_metric: 0.2006 - loss: 0.4340 - pixel_accuracy: 0.5664 - val_dice_coef: 0.5627 - val_iou_metric: 0.1905 - val_loss: 0.4373 - val_pixel_accuracy: 0.5628 - learning_rate: 1.0000e-04\n",
      "Epoch 5/7\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 2s/step - dice_coef: 0.5671 - iou_metric: 0.2055 - loss: 0.4329 - pixel_accuracy: 0.5674 - val_dice_coef: 0.5832 - val_iou_metric: 0.2112 - val_loss: 0.4168 - val_pixel_accuracy: 0.5837 - learning_rate: 1.0000e-04\n",
      "Epoch 6/7\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 2s/step - dice_coef: 0.5720 - iou_metric: 0.2104 - loss: 0.4280 - pixel_accuracy: 0.5723 - val_dice_coef: 0.5746 - val_iou_metric: 0.2112 - val_loss: 0.4254 - val_pixel_accuracy: 0.5750 - learning_rate: 1.0000e-04\n",
      "Epoch 7/7\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 2s/step - dice_coef: 0.5788 - iou_metric: 0.2138 - loss: 0.4212 - pixel_accuracy: 0.5790 - val_dice_coef: 0.5889 - val_iou_metric: 0.2134 - val_loss: 0.4111 - val_pixel_accuracy: 0.5891 - learning_rate: 1.0000e-04\n",
      "\n",
      ">> Modèle unet_vgg16 sauvegardé dans : c:\\Users\\vicau\\P8OC\\models\\unet_vgg16.keras\n",
      "===== Entraînement dédié unet_vgg16 terminé =====\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau  # type: ignore\n",
    "\n",
    "VGG_NAME = \"unet_vgg16\"\n",
    "EPOCHS_VGG16 = 7  # à ajuster \n",
    "\n",
    "vgg_model = models_dict[VGG_NAME]\n",
    "\n",
    "print(f\"\\n===== Entraînement dédié du modèle {VGG_NAME} (epochs = {EPOCHS_VGG16}) =====\")\n",
    "\n",
    "checkpoint_path = MODELS_DIR / f\"{VGG_NAME}.keras\"\n",
    "\n",
    "# Callbacks : early stopping + réduction du LR + sauvegarde du meilleur modèle\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    patience=8,\n",
    "    restore_best_weights=True,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    factor=0.3,\n",
    "    patience=4,\n",
    "    min_lr=1e-6,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=str(checkpoint_path),\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_best_only=True,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "vgg_model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=EPOCHS_VGG16,\n",
    "    callbacks=[early_stop, reduce_lr, checkpoint],\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "print(f\"\\n>> Modèle {VGG_NAME} sauvegardé dans : {checkpoint_path}\")\n",
    "print(\"===== Entraînement dédié unet_vgg16 terminé =====\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651185dc",
   "metadata": {},
   "source": [
    "## Validation intermédiaire (avant évaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "463ded9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Validation intermédiaire : chargement + inférence ===\n",
      "Batch validation chargé :\n",
      " - X_val_batch : (4, 256, 512, 3)\n",
      " - y_val_batch : (4, 256, 512, 8)\n",
      "\n",
      "--> Vérification du modèle : unet_mini\n",
      "   OK - modèle chargé\n",
      "   Forme des prédictions : (4, 256, 512, 8)\n",
      "\n",
      "--> Vérification du modèle : unet_mobilenetv2\n",
      "   OK - modèle chargé\n",
      "   Forme des prédictions : (4, 256, 512, 8)\n",
      "\n",
      "--> Vérification du modèle : unet_resnet50\n",
      "   OK - modèle chargé\n",
      "   Forme des prédictions : (4, 256, 512, 8)\n",
      "\n",
      "--> Vérification du modèle : unet_vgg16\n",
      "   OK - modèle chargé\n",
      "   Forme des prédictions : (4, 256, 512, 8)\n",
      "\n",
      "=== Validation intermédiaire OK : prêts pour l'évaluation finale ===\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from src.metrics import dice_loss, iou_metric, dice_coef, pixel_accuracy\n",
    "\n",
    "print(\"\\n=== Validation intermédiaire : chargement + inférence ===\")\n",
    "\n",
    "X_val_batch, y_val_batch = val_gen[0]\n",
    "\n",
    "print(\"Batch validation chargé :\")\n",
    "print(\" - X_val_batch :\", X_val_batch.shape)\n",
    "print(\" - y_val_batch :\", y_val_batch.shape)\n",
    "\n",
    "# Modèles à vérifier = ceux du screening + VGG16\n",
    "models_to_check = list(epochs_plan.keys()) + [\"unet_vgg16\"]\n",
    "\n",
    "for name in models_to_check:\n",
    "\n",
    "    model_path = MODELS_DIR / f\"{name}.keras\"\n",
    "    print(f\"\\n--> Vérification du modèle : {name}\")\n",
    "\n",
    "    assert model_path.exists(), f\"ERREUR : fichier manquant -> {model_path}\"\n",
    "\n",
    "    model = tf.keras.models.load_model(\n",
    "        model_path,\n",
    "        custom_objects={\n",
    "            \"dice_loss\": dice_loss,\n",
    "            \"iou_metric\": iou_metric,\n",
    "            \"dice_coef\": dice_coef,\n",
    "            \"pixel_accuracy\": pixel_accuracy,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    preds = model.predict(X_val_batch, verbose=0)\n",
    "\n",
    "    print(\"   OK - modèle chargé\")\n",
    "    print(\"   Forme des prédictions :\", preds.shape)\n",
    "\n",
    "print(\"\\n=== Validation intermédiaire OK : prêts pour l'évaluation finale ===\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa90906e",
   "metadata": {},
   "source": [
    "# Évaluation finale des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "134d7e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Évaluation finale des modèles ===\n",
      "\n",
      "--> Évaluation du modèle : unet_mini\n",
      "   OK - Évaluation terminée\n",
      "\n",
      "--> Évaluation du modèle : unet_mobilenetv2\n",
      "   OK - Évaluation terminée\n",
      "\n",
      "--> Évaluation du modèle : unet_resnet50\n",
      "   OK - Évaluation terminée\n",
      "\n",
      "--> Évaluation du modèle : unet_vgg16\n",
      "   OK - Évaluation terminée\n",
      "\n",
      "=== Tableau comparatif final ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>dice_coef</th>\n",
       "      <th>iou_metric</th>\n",
       "      <th>pixel_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>unet_mini</th>\n",
       "      <td>0.857398</td>\n",
       "      <td>0.142602</td>\n",
       "      <td>0.027295</td>\n",
       "      <td>0.142602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unet_mobilenetv2</th>\n",
       "      <td>0.257861</td>\n",
       "      <td>0.742139</td>\n",
       "      <td>0.373283</td>\n",
       "      <td>0.743483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unet_resnet50</th>\n",
       "      <td>0.857398</td>\n",
       "      <td>0.142602</td>\n",
       "      <td>0.027295</td>\n",
       "      <td>0.142602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unet_vgg16</th>\n",
       "      <td>0.411067</td>\n",
       "      <td>0.588933</td>\n",
       "      <td>0.213391</td>\n",
       "      <td>0.589134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      loss  dice_coef  iou_metric  pixel_accuracy\n",
       "unet_mini         0.857398   0.142602    0.027295        0.142602\n",
       "unet_mobilenetv2  0.257861   0.742139    0.373283        0.743483\n",
       "unet_resnet50     0.857398   0.142602    0.027295        0.142602\n",
       "unet_vgg16        0.411067   0.588933    0.213391        0.589134"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from src.metrics import dice_loss, iou_metric, dice_coef, pixel_accuracy\n",
    "\n",
    "print(\"\\n=== Évaluation finale des modèles ===\")\n",
    "\n",
    "evaluation_results = {}\n",
    "\n",
    "models_to_evaluate = list(epochs_plan.keys()) + [\"unet_vgg16\"]\n",
    "\n",
    "for name in models_to_evaluate:\n",
    "    model_path = MODELS_DIR / f\"{name}.keras\"\n",
    "    print(f\"\\n--> Évaluation du modèle : {name}\")\n",
    "\n",
    "    model = tf.keras.models.load_model(\n",
    "        model_path,\n",
    "        custom_objects={\n",
    "            \"dice_loss\": dice_loss,\n",
    "            \"iou_metric\": iou_metric,\n",
    "            \"dice_coef\": dice_coef,\n",
    "            \"pixel_accuracy\": pixel_accuracy,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    scores = model.evaluate(val_gen, verbose=0, return_dict=True)\n",
    "\n",
    "    evaluation_results[name] = {\n",
    "        \"loss\": scores.get(\"loss\", None),\n",
    "        \"dice_coef\": scores.get(\"dice_coef\", None),\n",
    "        \"iou_metric\": scores.get(\"iou_metric\", None),\n",
    "        \"pixel_accuracy\": scores.get(\"pixel_accuracy\", None),\n",
    "    }\n",
    "\n",
    "    print(\"   OK - Évaluation terminée\")\n",
    "\n",
    "df_eval = pd.DataFrame(evaluation_results).T\n",
    "print(\"\\n=== Tableau comparatif final ===\")\n",
    "df_eval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa9564",
   "metadata": {},
   "source": [
    "# Choix du modèle de segmentation pour l’API et l’application\n",
    "\n",
    "Les quatre architectures ont été évaluées sur le jeu de validation (`val_gen`) à l’aide des métriques suivantes : loss, Dice Coefficient (`dice_coef`), IoU (`iou_metric`) et Pixel Accuracy (`pixel_accuracy`).\n",
    "\n",
    "Résultats obtenus :\n",
    "\n",
    "| Modèle           | loss     | dice_coef | iou_metric | pixel_accuracy |\n",
    "|------------------|----------|-----------|------------|----------------|\n",
    "| unet_mini        | 0.857398 | 0.142602  | 0.027295   | 0.142602       |\n",
    "| unet_mobilenetv2 | 0.257861 | 0.742139  | 0.373283   | 0.743483       |\n",
    "| unet_vgg16       | 0.411067 | 0.588933  | 0.213391   | 0.589134       |\n",
    "| unet_resnet50    | 0.857398 | 0.142602  | 0.027295   | 0.142602       |\n",
    "\n",
    "---\n",
    "\n",
    "## Analyse des modèles\n",
    "\n",
    "**UNet Mini**  \n",
    "UNet Mini constitue un modèle léger utilisé principalement comme baseline. Les performances obtenues restent limitées (Dice ≈ 0.14, IoU très faible), ce qui le rend insuffisant pour une segmentation sémantique exploitable. Il reste néanmoins utile pour valider le bon fonctionnement du pipeline de données et d’entraînement.\n",
    "\n",
    "**UNet + MobileNetV2**  \n",
    "Cette architecture obtient les meilleures performances numériques dans ce cadre expérimental, avec un Dice élevé (≈ 0.74) et une IoU significative (≈ 0.37). Ces résultats mettent en évidence l’efficacité de MobileNetV2 pour l’extraction de caractéristiques visuelles, notamment dans des configurations d’entraînement courtes. Toutefois, ce modèle se montre plus sensible aux réglages fins et à la stabilité de l’entraînement.\n",
    "\n",
    "**UNet + VGG16**  \n",
    "Le modèle UNet + VGG16 présente des performances solides et régulières (Dice ≈ 0.59, IoU ≈ 0.21, Pixel Accuracy ≈ 0.59), associées à une loss modérée. Bien que ses scores soient inférieurs à ceux de MobileNetV2 dans cette phase, le comportement du modèle apparaît plus stable et plus prévisible. L’architecture VGG16 permet une extraction hiérarchique des features et facilite l’analyse des résultats et des erreurs de segmentation.\n",
    "\n",
    "**UNet + ResNet50**  \n",
    "Les performances observées restent faibles (Dice ≈ 0.14, IoU ≈ 0.03). Dans le cadre d’un entraînement limité et réalisé sur CPU, le modèle n’a pas convergé de manière satisfaisante. La profondeur et la complexité de ResNet50 nécessiteraient des ressources et un temps d’entraînement plus importants pour être pleinement exploitées.\n",
    "\n",
    "---\n",
    "\n",
    "## Justification du choix final\n",
    "\n",
    "Bien que le modèle **UNet + MobileNetV2** présente les meilleures performances numériques à ce stade, le choix s’est porté sur **UNet + VGG16** comme modèle principal du projet. Cette décision repose sur plusieurs critères complémentaires aux métriques brutes.\n",
    "\n",
    "L’architecture VGG16 offre un comportement d’entraînement plus stable et prévisible, ce qui facilite l’analyse des résultats et l’itération sur le modèle. Dans un contexte de déploiement contraint sur Heroku, la robustesse du chargement du modèle et la fiabilité de son exécution constituent des critères déterminants. VGG16, largement documenté et éprouvé, limite les risques de comportements inattendus en production.\n",
    "\n",
    "Ce choix est également cohérent avec une expérience préalable sur des projets personnels utilisant VGG16, permettant une meilleure maîtrise de son fonctionnement et de ses limites. Enfin, l’objectif du projet étant de démontrer une chaîne complète de segmentation d’images, de l’entraînement au déploiement via une API et une application web, la cohérence globale et la fiabilité du système ont été privilégiées.\n",
    "\n",
    "Ainsi, **UNet + VGG16** est retenu comme **modèle principal** pour :\n",
    "- l’API de prédiction,\n",
    "- l’application web de démonstration,\n",
    "- les visualisations (image, masque réel, masque prédit) présentées dans la note technique.\n",
    "\n",
    "Les autres modèles restent intégrés au projet à des fins de comparaison et d’analyse méthodologique.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
